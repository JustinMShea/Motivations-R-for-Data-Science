---
title: "Motivations: R for Data Science"
author: "Justin M Shea"
date: "updated `r format(Sys.time(), '%d %B %Y')`"
output:
  ioslides_presentation:
    widescreen: true
    logo: images/Rlogo436x338.png

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Outline

- What is R and Data Science?

- What languages (tools) are used in Data Science now? 

- Where did R and Data Science come from? 

- What does a Data Scientist's Workflow in R look like?

- How to learn more?



## So what is Data Science?{.columns-2 .smaller}

**_Blind Men Appraising an Elephant_**, by Ohara Donshu

Like _Blind Men Appraising an Elephant_, its natural for experts spanning a diverse 
range of disciplines to view Data Science from the perspective of their own domain.

However, like Ohara Donshu, those interested in Data Science needs to be able to 
view the subject from a far, and understand many parts make a whole.


```{r, out.width = "550px"}
path <- paste0(getwd(), "/images/Blind_Men_Appraising.jpg")
knitr::include_graphics(path)
```

[Image Source: wiki/Blind_men_and_an_elephant](https://en.wikipedia.org/wiki/Blind_men_and_an_elephant)



## Data Science Venn Diagram {.columns-2 .smaller}

Conceived by _Drew Conway_, after organizing the [first Strata conference in 2011](https://conferences.oreilly.com/strata/strata2011) with a group of NYC's most sophisticated thinkers on all things data. 
```{r, out.width = "400px"}
path <- paste0(getwd(), "/images/Data_Science_VD.png")
knitr::include_graphics(path)
```

[Image Source: drewconway.com](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram)


**Data Hacking Skills**: The laws of electronic data handling. The ability to programmatically acquire data from the web or a database, manipulate data files at the command-line, understand vectorized operations, and use version control. Before you run the first model or visualization.

**Math & Statistics**: Once you have acquired and cleaned the data, the next step is to extract insight from it. Learn best practices gained from thousands of years of successes and failures. Utilize data visualizations as your guide.

**Substantive Expertise**: Domain expertise is critically important. Science is about discovery and building knowledge, which requires some motivating questions about the world and hypotheses that can be brought to electronic data and tested with statistical methods. 



## Data Science Venn Diagram 2 {.columns-2 .smaller}


Conceived by _Drew Conway_, after organizing the [first Strata conference in 2011](https://conferences.oreilly.com/strata/strata2011) with a group of NYC's most sophisticated thinkers on all things data. 
```{r, out.width = "400px"}
path <- paste0(getwd(), "/images/Data_Science_VD.png")
knitr::include_graphics(path)
```

[Image Source: drewconway.com](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram)

**Danger Zone**: Hacking skills plus substantive expertise. This is where those who "know enough to be dangerous," live. Computer Science majors beware, knowledge of statistical methods is not optional!

**Traditional Research**: Substantive expertise plus math and statistics knowledge. Doctoral level researchers spend most of their time acquiring expertise in these areas, but little time learning about technology. Learn software development skills to reproduce your work, collaborate with others, and productionize it.

**Machine Learning**: Data Hacking skills plus math and statistics. Powerful, 
but "discoveries" may prove inhumane or counterproductive to project goals. Learn the key issues in your field if you don't want to inadvertently create [Weapons of Math Destruction](https://weaponsofmathdestructionbook.com)

## Origins of the term "Data Scientist" {.columns-2 .smaller}

Origins seem to stem from series of conversations in 2008 between [DJ Patil](https://twitter.com/dpatil) (the creator of LinkedIn's Data Science Team and first U.S. Chief Data Scientist under the Obama administration) and [Jeff Hammerbacher](https://twitter.com/hackingdata) (creator of Facebook's Data Science team).

_"Although the term "data science" has a long history usually referring to business intelligence the term "data scientist" appears to be new. Jeff and I have been asking if anyone else has used this term before we coined it, but we've yet to find anyone who has."_  -  DJ Patel, then Chief Scientist at LinkedIn

[Source: Building Data Science Teams ](http://radar.oreilly.com/2011/09/building-data-science-teams.html)

What makes a good data scientist according to DJ?

- **Technical expertise**: the best data scientists typically have deep expertise in some scientific discipline.

- **Curiosity:** a desire to go beneath the surface and discover and distill a problem down into a very clear set of hypotheses that can be tested.

- **Storytelling:** the ability to use data to tell a story and to be able to communicate it effectively.

- **Cleverness:** the ability to look at a problem in different, creative ways.


 
##  The Rise of the Data Scientist {.smaller}

**A Domain Problem**: _Facebook had saturated the student population at nearly all of the colleges where it was available, but there were still some colleges where the product had never taken off. What distinguished these laggard networks from their more successful peers, and what could be done to stimulate their success?_

Jeff Hammerbacher, a kid from Fort Wayne, IN.

_"When I interviewed at Facebook in February 2006, they were actively looking to answer these questions. I studied mathematics in college and had been working for a nearly a year on Wall Street, building models ... **I had some experience coding and a dismal GPA**. Despite my potentially sub-optimal background, Facebook made me an offer to join as a Research Scientist."_


From ["Information Platforms as Dataspaces" PDF](https://lintool.github.io/UMD-courses/bigdata-2015-Spring/content/Hammerbacher_2009.pdf)


## So What's a Data Scientist again? {.smaller}

Some of the fields most talented people don't identify with the newer jargon,
reflecting origins and a mix of skills far older than new labels.

```{r, out.width = "900px"}
path <- paste0(getwd(), "/images/Josh-U-quote.png")
knitr::include_graphics(path)
```

Josh has it all: **Substantive expertise, Math & Statistics, and Data Hacking Skills:**

- M.A. Economics, 2003, University of Missouri-St. Louis. Econometric (statistics) focus.

- Mastery of `R`, `C`/`C++`, `C#`, `Java`, and `Unix` command line.

- Modeled High-Frequency financial data at the Federal Reserve Bank of St. Louis in 2004, went on to implement risk and trading systems at Wells Fargo and DV Trading (high-frequency trading firm).

- Author/contributor to high performance time series packages used across industries.

 
# Data Science Tools


## European Spreadsheet Risks Interest Group

Wait...this group even exists? Risks include "Systemic Financial Failure"???

```{r, out.width = "850px", fig.align="center"}
path <- paste0(getwd(), "/images/EuSpRiG.png")
knitr::include_graphics(path)
```

[Source: www.euspring.org](http://www.eusprig.org)

## They have a Horror Stories Page Too! {.smaller}

The Famous Rogoff-Reinhart excel error omits Denmark, Canada, Belgium, Austria, and Australia out of a key average, changing economic growth from positive to negative. The paper had been cited by conservative politicians to justify cuts to social programs. Colbert has a field day.

```{r, out.width = "850px", fig.align="center"}
path <- paste0(getwd(), "/images/Error-RR.png")
knitr::include_graphics(path)
```

http://www.eusprig.org/horror-stories.htm

## Good solutions to make spreadsheets safer {.columns-2 .smaller}

- be consistent

- write dates like YYYY-MM-DD

- don't leave any cells empty

- put just one thing in a cell

- organize data with subjects as rows and variables as columns

- create a data dictionary

- don't include calculations in the raw data files

- don't use font color or highlighting as data

- choose good names for things

- make backups

- use data validation to avoid data entry errors

- save the data in plain text file

```{r, out.width = "350px", fig.align="center"}
path <- paste0(getwd(), "/images/Broman-Woo.png")
knitr::include_graphics(path)
```

[Source: Data organization in spreadsheets](https://peerj.com/preprints/3183/)

## ...but move beyond spreadsheets

_Although spreadsheet programs are used for small "scratchpad" applications, they are also used to develop many large applications. In recent years, we have learned a good deal about the errors that people make when they develop spreadsheets. In general, errors seem to occur in a few percent of all cells, meaning that for large spreadsheets, the issue is how many errors there are, not whether an error exists. These error rates, although troubling, are in line with those in programming and other human cognitive domains. In programming, we have learned to follow strict development disciplines to eliminate most errors._ - Raymond Panko 


[Source: "What We Know About Spreadsheet Errors."](http://panko.shidler.hawaii.edu/SSR/Mypapers/whatknow.htm) 

## Burchworks Survey on Data Science Tools {.columns-2 .smaller}

Burch works is a decades old Chicago based recruiting firm specializing in analytics 
talent. The Survey of about 1000 is a "guesstimate"", reflecting their client base of 
traditional Chicago companies.


**Results indicate `R` the most popular, with `SAS` 2nd, and `Python` growing enormously. Reflects a general shift to programming languages and away from commercial analytics products.**

Ironically, they don't use best practices in Data Science, Reproducible Research,
or Survey methods, creating much ire among data professionals every year! 

However, "Interesting" because its mostly local!


[Source: Full '2017 Burtchworks Survey' HERE](https://www.burtchworks.com/2017/06/19/2017-sas-r-python-flash-survey-results/)

```{r, out.width = "500px"}
path <- paste0(getwd(), "/images/burtch-Preference-over-four-years.png")
knitr::include_graphics(path)
```

## IEEE, general language rankings {.columns-2 .smaller}


Amazingly, in the wider world of programming languages, the `R Statistical Programming language`,
created by Data Scientists for **Data Scientists**, makes the "top 10" general programming language cut, reflecting it's growing use across scientific domains, business sectors, and Enterprises. It is very surprising for a specialized language to rank so high.


`Python`'s popularity is mainly due to it's high use in Web site development, DevOps, and the west coast technology industry in general. Technology is the fastest growing sector in the economy at present. Use in web development and recent adaptation in **Data Science** has put it on top.


`C` and `C++` remains a stalwarts, and underpins `R`. 
Side note: R package developers use `C` and `C++` to speed up `R`.

```{r, out.width = "500px"}
path <- paste0(getwd(), "/images/ieee-10.png")
knitr::include_graphics(path)
```

[Source: IEEE 2017 top programming languages](https://spectrum.ieee.org/computing/software/the-2017-top-programming-languages)
[Source: IEEE Design, Methods, and Data Sources](https://spectrum.ieee.org/ns/IEEE_TPL_2017/methods.html)


## Stack overflow analysis of Top Languages {.smaller}

Stack Overflow graphs done with `R` helps us visualize that `R` is growing fast! [Source](https://stackoverflow.blog/2017/09/14/python-growing-quickly
)

```{r, out.width = "500px", fig.align="center"}
path <- paste0(getwd(), "/images/SO-growth.png")
knitr::include_graphics(path)
```

## Scripting languages: the core of Data Science
High Level languages like `R` interface with low level languages and Numerical computing libraries. 

They are easier to program so analysts can fail fast on their research and start again. Most research ideas and science fail, so emphasis is placed on how fast one can churn them out.

Package ecosystem inherent to these languages allows scientists to contribute their work, with software they write. This has led to an explosion in the sharing of work across scientific disciplines.


## Python: From Web sites to Data Science {.columns-2 .smaller}

According to Stack Overflow:

_The fastest-growing use of `Python` is for data science, machine learning and academic research. This is particularly visible in the growth of the pandas package, which is the fastest-growing `Python`-related tag on the site._

While Data Science with Python is really growing, its current popularity stems from 
its combined presence in web development, Sysadmin, and DevOps.


**Data Science:** `pandas`, `numpy`, `matplotlib`

**Web Developer:** `Javascript`, `django`, `HTML`

**Sysadmin/DevOps:** `Linux`, `Bash`, or `Windows`.


[Source: Stack Overflow, Incredible growth of python](stackoverflow.blog/2017/09/06/incredible-growth-python)



```{r, out.width = "500px"}
path <- paste0(getwd(), "/images/SO-Py.png")
knitr::include_graphics(path)
```


## R: created for Data Science, Growth by industry {.columns-2 .smaller}

_`R` is growing across industries, but visits are generally growing faster in industries where it was already more heavily visited._

_`R` is most visited from universities, where it's a common choice for academic research, especially in the social sciences and biology._

_The industry with the second-highest share of `R` visitors, by a close margin, is healthcare. That probably won't come as a surprise to biostatisticians, since `R` is the tool of choice for many statistical methods necessary for clinical studies and bioinformatics._ (R is mature. [It is used and OK'd by the FDA for drug trials](http://blog.revolutionanalytics.com/2012/06/fda-r-ok.html)


_One industry that doesn't visit as much of `R`, relative to other technologies, is tech: software and web companies._ 


[Source: Stack Overflow, impressive growth of R](https://stackoverflow.blog/2017/10/10/impressive-growth-r/)



```{r, out.width = "500px"}
path <- paste0(getwd(), "/images/SO-R.png")
knitr::include_graphics(path)
```

## Task Views in R {.columns-2 .smaller}

Years ago, I discovered `R` because I needed to use the most cutting edge, statistical modeling libraries available. With over 12,000 packages available beyond base `R` on CRAN alone, the community organizes noteworthy and mature approaches by research domain. They can ALL be installed with a few commands.

```{r, out.width = "400px"}
path <- paste0(getwd(), "/images/Task-View-1.png")
knitr::include_graphics(path)
```


```{r, out.width = "400px"}
path <- paste0(getwd(), "/images/Task-View-2.png")
knitr::include_graphics(path)
```

https://cran.r-project.org/web/views/


## R the least disliked Data Science Language! {.columns-2 .smaller}

When choosing a first scripting language, narrow it down. Research your domain or industry and discover what the people and companies you are inspired by are using. Have your values and interests drive the process.

_I've been a lifelong Mac and UNIX user, and nearly all of my programming in college and graduate school was centered around Python and R. Despite that, I was happy to join a company with a .NET stack, and I'm glad I did— because I loved the team, the product, and the data._ _**I can't speak for anyone else, but I'm glad I defined myself in terms of what work I wanted to do, and not something I wanted to avoid.**_ 

**David Robinson, Data Scientist** @[stackoverflow](https://stackoverflow.com/). **Prefers R for Data Science**

```{r, out.width = "600px"}
path <- paste0(getwd(), "/images/SO-wars.png")
knitr::include_graphics(path)
```

[Source: Disliked Programming Languages](https://stackoverflow.blog/2017/10/31/disliked-programming-languages/)


# (Some) R and Data Science History


## 1880: Statistician solves a Big Data problem {.columns-2 .smaller}

**1880** Since its inception in 1790, the U.S. census has expanded dramatically. It takes 8 years to process.

Herman Hollerith, **_a Statistician of the 1880 census_**, sets out to process it faster. Inspired by railway conductors, he creates punch cards to collect data. Inserted into a machine, the holes in each card allow an electrical circuit to complete, advancing a counting wheel for each topic and card.

**1890** The first census to use Hollerith's Tabulating Machine is a success, completing in just 1 year. 

**1896** Hollerith forms the "Tabulating Machine Company", which later becomes "International Business Machines" better known today as "IBM".


```{r, out.width = "400px"}
path <- paste0(getwd(), "/images/HollerithMachine.jpg")
knitr::include_graphics(path)
```

Sources: [Census.gov/history](https://www.census.gov/history/www/census_then_now/notable_alumni/herman_hollerith.html) and pg. 36 _The Innovators_, Walter Isaacson



## John Tukey, a Data Science pioneer {.columns-2 .smaller}

**1962** [_The Future of Data Analysis_](https://www.jstor.org/stable/2237638) 

**_I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data._**

**1964** [_The technical tools of statistics_](http://ect.bell-labs.com/sl/tukey/memo/techtools.html) 

**_Today, software and hardware together provide far more powerful factories than most statisticians realize, factories that many of today's most able young people find exciting and worth learning about on their own._**



**1939** Earns a Ph.D Mathematics, Princeton

**1947** Working with [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) at **AT&T Bell Laboratories**, Tukey Invents the term "bit"

**1956**  Sets up Princeton's Statistical Techniques Research Group (STRG)

**1961** Promoted to Executive Director-Research Information Sciences at **AT&T Bell Laboratories**

**1965** 1st Chair of Princeton's Statistics Department

**1965** Published influential paper on the Fast Fourier Transformation with J.W. Cooley, leading to the rapid development of digital signal processing

[Source: Memories of John W. Tukey](http://ect.bell-labs.com/sl/tukey/)


## 1977: John Tukey's Exploratory Data Analysis {.columns-2 .smaller}

Spending over 30 years developing ground breaking methods still in use today,
Tukey concludes visualization as one of the most important parts of data analysis.

**"Since the aim of exploratory data analysis is to learn what seems to be, it should be no surprise that pictures play a vital role in doing it well. There is nothing better for making you think of questions you had forgotten to ask (even mentally)"** - John Tukey

Tukey's championing of EDA encouraged the development of statistical computing packages, especially `S` at Bell Labs. The `S` programming language inspired the systems `S-PLUS` and **`R`**. This family of statistical-computing environments featured vastly improved dynamic visualization capabilities, which allowed statisticians to identify outliers, trends and patterns in data that merited further study.



```{r, out.width = "300px"}
path <- paste0(getwd(), "/images/Tukey-EDA.jpg")
knitr::include_graphics(path)
```

## Anscombe and his 1973 data quartet {.smaller .columns-2}

Tukey and Anscombe often spoke of graphics as analysis.
Anscombe's plots illustrates. All 4 data sets return nearly identical linear regression coefficient estimates. However, viewing graphs of the data reveals each is quite different.


```{r, warning=FALSE, out.width= "450px"}
data("anscombe")

anscombe.1 <- data.frame(Set = "Anscombe 1", x = anscombe[["x1"]], y = anscombe[["y1"]])
anscombe.2 <- data.frame(Set = "Anscombe 2", x = anscombe[["x2"]], y = anscombe[["y2"]])
anscombe.3 <- data.frame(Set = "Anscombe 3", x = anscombe[["x3"]], y = anscombe[["y3"]])
anscombe.4 <- data.frame(Set = "Anscombe 4", x = anscombe[["x4"]], y = anscombe[["y4"]])

anscombe.data <- rbind(anscombe.1, anscombe.2, anscombe.3, anscombe.4)

model1 <- lm(y ~ x, subset(anscombe.data, Set == "Anscombe 1"))
model2 <- lm(y ~ x, subset(anscombe.data, Set == "Anscombe 2"))
model3 <- lm(y ~ x, subset(anscombe.data, Set == "Anscombe 3"))
model4 <- lm(y ~ x, subset(anscombe.data, Set == "Anscombe 4"))

library(ggplot2)
gg <- ggplot(anscombe.data, aes(x = x, y = y)) + 
             geom_point(color = "black", size = 2) + 
             facet_wrap(~Set, ncol=2) + 
             geom_smooth(method="lm", se = FALSE, size=1)
              
gg + theme_bw()
```

[Original Paper PDF: _Graphs in Statistical Analysis"](http://www.sjsu.edu/faculty/gerstman/StatPrimer/anscombe1973.pdf)

```{r, out.width = "400px"}
path <- paste0(getwd(), "/images/anscombe.png")
knitr::include_graphics(path)
```


## Statistical/Machine Learning {.columns-2 .smaller}

Bridges statistics and computer science. Many early innovators among `S` & `R` communities at Stanford and Bell Labs.

**1984** Classification and Regression Trees

**1994** Bagging Predictors, Leo Breiman

**1995** Random Decision Forests, Tin Kam Ho at **Bell**

**1999** `Random Forest` - _Random Features_, Breiman

**2000** `AdaBoost` Additive Logistic Regression: A Statistical View of Boosting. Jerome Friedman, Trevor Hastie, Rob Tibshirani.

**2001** [_The Two Cultures_](https://projecteuclid.org/euclid.ss/1009213726), Leo Breiman.

**2002** `randomForest` R package. Breiman's Fortran ported to R by Andy Liaw and Matthew Wiener.

[Revolution Analytics: History of Ensemble Methods](http://blog.revolutionanalytics.com/2014/03/a-thumbnail-history-of-ensemble-methods.html)

```{r, out.width = "300px"}
path <- paste0(getwd(), "/images/CART-Book.jpg")
knitr::include_graphics(path)
```


# (Some) Data Analysis Software History

## Statistical Analsyis System (SAS) {.columns-2 .smaller}

**1966** The development of SAS began in 1966 after North Carolina State University rehired Anthony Barr to program his analysis of variance and regression software. The project was funded by the **National Institute of Health** to analyze agricultural data to improve crop yields

**1972** After issuing the first release of SAS, the project lost its funding. According to James Goodnight (the other project leader) this was because NIH only wanted to fund projects with medical applications. There was some disagreement on what the funding was used for.

**1976** Barr, Goodnight, Sall, and Helwig removed the project from North Carolina State and privatize it, incorporating it into SAS Institute, Inc.

```{r, out.width = "300px"}
path <- paste0(getwd(), "/images/SAS.png")
knitr::include_graphics(path)
```

**1980s - present** Private company grows to 3.2 Billion in annual sales and founders get rich! 



However extremely large licensing costs and proprietary source code doesn't allow leading researchers in academia or industry to adapt the software to current needs. As Data Analysis evolves into an interdisciplinary field, begins loses market share to free open source alternatives that have more flexibility to evolve quickly.  


## 1977: Bell Labs, S, and R {.columns-2 .smaller}

*2000* John Chambers, core creator of the S Language

_The essence of what we took from him in thinking about S, for example, can be summarized in another of his famous one-liners, which goes roughly like this:_

**_Better to have an approximate answer to the right question than a precise answer to the wrong question._**

_To me, this means that the data and the real goals of the analysis should suggest what questions need posing. A convenient or conventional summary is often not good enough. In this sense, I would say that enabling users to ask the "right" question (and to get a usable answer) is the most central design principle behind S, and behind nearly all the other research here on statistical software, before and since._ 

```{r, out.width = "150px"}
path <- paste0(getwd(), "/images/201309_RandS_timeline.jpg")
knitr::include_graphics(path)
```


## R (and S) for Humans, always the goal! {.columns-2 .smaller}

**1975** Richard Becker and John Chambers, both statisticians working on data analysis at Bell Labs and both computer scientists, were frustrated with having to interrupt their data analysis to do the required programming. 

They wanted a more interactive process, where they could just type an expression at the command line and get back an immediate response, or better yet, a bar graph or other visual.

**1977: _Analalysis that was once difficult became easy in S_.**

**1991** `R`, based entirely on `S`, is created by Ross Ihaka and Robert Gentleman, borrowing all the hard-won lessons developed by the S community.

[Source: From S to R: 35 Years of Leadership in Statistical Computing](http://www.research.att.com/articles/featured_stories/2013_09/201309_SandR.html)

```{r, out.width = "150px"}
path <- paste0(getwd(), "/images/201309_RandS_timeline.jpg")
knitr::include_graphics(path)
```

## A rich history of extending R  {.columns-2 .smaller}

R succeeds today because it is very easy to extend. 

If the driving force behind S was interactivity, for R it was extensibility. Partly this extensibility stems from S, which was designed to interface easily with outside elements (specifically with subroutine libraries). R retains this interfacing capability so it easily incorporates native code as well as objects, analysis, and visualizations created in other programs, giving R users options outside of R. And R is extensible also because it's open source.

Objects created in other systems and then imported into R are accessed in the same way R objects are accessed. If one can write code in Java, C, C++, or other languages, mature bindings can use R to call the code directly, with little loss in speed or performance.


[Source: From S to R: 35 Years of Leadership in Statistical Computing](http://www.research.att.com/articles/featured_stories/2013_09/201309_SandR.html)


```{r, out.width = "150px"}
path <- paste0(getwd(), "/images/201309_RandS_timeline.jpg")
knitr::include_graphics(path)
```

## Python: From WebDev to Data Science

General discussion 

- Created in 1991 by Guido Von Rossum who wanted prettier `ABC` language.

- Mostly a webDev and SysAdmin language until the more recent release of `Pandas`, making the data.frame object available for the very first time. Author Wes McKinney works closely with senior developers in the `R` community who in the spirit of open source, show him the way.

- Python is also open source and has a large community of developers who help evolve it faster than any other data analysis software since `R`.

- Leap frogging ahead has left Python without many important graphical analysis tools and statistical/machine learning techniques that are still widely used in Data Analysis.

- A good language for learning general programming principals

# R and Data Science in Practice

## An R Projects workflow {.smaller}

```{r, out.width = "666px", fig.align="center"}
path <- paste0(getwd(), "/images/DS-Workflow.png")
knitr::include_graphics(path)
```
**First Mile**: _Import_ data and prepare for Analysis. Hadley Wickham's [_Tidy_ Data:](https://www.jstatsoft.org/article/view/v059i10) Each variable is a column, each observation is a row, and each type of observational unit is a table. Alot of time is spent here.

**Data Analysis**: _Transform_ data and create new variables with `log`, `exponents`, differencing, etc. _Visualizations_ help spot outliers, remaining data problems, and reveal insights to test with _Models_. Predictive modeling frameworks like [Max Kuhn's `caret`](https://topepo.github.io/caret/index.html) provide tools to quickly iterate over the entire process.

**Last Mile**: _Communicate_ results with visualizations and good writing, exporting to static or interactive documents, webpages, apps, or presentation slides. Packages like [R Markdown](https://rmarkdown.rstudio.com/gallery.html) provides the scaffolding.

[Image Source: R for Data Science](http://r4ds.had.co.nz/introduction.html)


## What about 'Big Data' and R? {.smaller .columns-2}

Szilard Pfafka: Physics PhD, chief (data) scientist @Epoch, and (visiting) UNCLA professor.

He points out that most people's data is just not that big. Less than 10% work with
data larger than 8 Terabytes.

```{r, out.width = "500px"}
path <- paste0(getwd(), "/images/szilard-data-size.png")
knitr::include_graphics(path)
```


[Szilard Pfafka Source: dataset-sizes-kdnuggets](https://github.com/szilard/dataset-sizes-kdnuggets)


## What about 'Big Data' and R?  {.smaller .columns-2}

...and he points out that processing power is getting bigger! 

R's ability to compute on big data scales with processing power. 

Processing power is increasing on the PC and cloud. Amazon AWS and Microsoft Azure provide access to lots of processing power for cheap.

This allows one to develop scripts locally on smaller sample data and then load scripts with
bigger data into the cloud.

[Check out Microsoft Azure and R here](https://azure.microsoft.com/en-us/)

[2017-n1 meetup on useing AWS and R here](https://github.com/Chicago-R-User-Group/2017-n1-Meetup/blob/master/Hultman%2C%20Keith-Setting%20Up%20RStudio%20on%20AWS.pdf)



```{r, out.width = "300px"}
path <- paste0(getwd(), "/images/szilard-ram-size.png")
knitr::include_graphics(path)
```


[Szilard Pfafka Source: dataset-sizes-kdnuggets](https://github.com/szilard/dataset-sizes-kdnuggets)






## How to learn more?

General discussion

- Foundations in Math and Statistics are important


- Datacamp.com is excellent for learning how to do specific things and learn the language of R (and Python too for the same money)

- Signup for meetups and go! They are free, keep you current.

